# ADR-001: Задание 1. Выбор решения для пакетной обработки данных
## Автор: Сурков Александр
## Дата: 12.09.2025

## 1. Контекст
Маркетинговый отдел планирует расширенно обрабатывать данные клиентов, объединяя их из разных источников, и формировать отчёты. В работе используются данные из разных источников: файловое хранилище CSV-файлов, содержащих статусы доставок пользователей, таблицы PostgreSQL с информацией о заказах, данные о платежах по заказам, расширенные данные о пользователях и данные из Kafka с цепочкой событий по модификации заказов.  

Текущая система обработки не справляется с нагрузкой и запрашиваемым функционалом. 

## 2. Требования
Необходимо выбрать и обосновать технологическое решение для пакетной обработки данных.   

Логика обработки подразумевает возможность гибко формировать пайплайна, интегрировать его с внешними API, BigQuery, Redshift, Kafka и Spark, а также поддерживать встроенный мониторинг и оповещения. Ожидаемый объём обрабатываемых данных за один запуск пайплайна — около 1 млн.   

Что должно быть в выборе решения:
- Опишите, как выбранное решение можно интегрироваться с BigQuery, Redshift, Kafka и Spark, отметив, есть ли для него готовые модули, которые ускорят разработку.
- Решение поддерживает возможность ветвления, условных операторов и event-triggers?
- Можно ли использования из коробки fallback-logic, retry и отправку email-уведомлений?
- Обоснуйте, как решение будет развернуто в облачной среде.

## 3. Решение
Для пакетной обработки данных видятся два варианта:
- Spring Batch. Входит в экосистему Java. Это надежное решение, с хорошими возможностями для тестирования. Совместно со Spring Cloud Data Flow (SCDF) позволяет разворачивать задачи локально или в Kubernetes.
- Apache Airflow. Специализированное решение для ETL процессов. Имеет широкие возможности для интеграции с различными источниками данны.

В требованиях говорится, что уже существует система обработки(непонятно какая), поэтому будем исходить из современных тенденций в построении ETL процессов. А они связаны с Apache Airflow.

Такой выбор предполагает, что ETL процессы будут написаны c использованием Python и Spark. Это привычные инструменты в мире аналитики и ML.

Аргументы в пользу выбора Apache Airflow:
- Возможность формирования сложных пайплайнов с поддержкой ветвлений и условий
- возможность интеграции с внешними сервисами, такими как REST API, базы данных и файловые хранилища(PostgreSQL, BigQuery, Redshift, Kafka, Spark и т.д.)
- Event-triggered workflows - запуск пайплайнов по событиям
- Retry logic и fault tolerance: перезапуск задач в случае неудачи, с настраиваемым числом попыток и интервалами ожидания
- Мониторинг и оповещение: слежение за выполнения пайплайнов и ведение журнала событий. Интегрируется с системами отправки уведомлений(Email)
- Производительность и масштабируемость
- Развертывание в облачной среде: Airflow можно развернуть публичном облаке (AWS, GCP, Azure) или самостоятельно разворачивая кластер в Kubernetes с помощью helm-чартов


## 4. Альтернативы
При определенных условиях можно было бы выбрать Spring Batch + SCDF. Такой выбор был бы оправдан если бы:
- разработчики владели Java и не хотели погружаться в изучение Python
- ETL процессы были относительно стабильными: требования менялись редко и новые процессы появлялись не часто

Еще одним альтернативным вариантом мог бы стать Kubeflow Pipelines. У него следующие особенности по сравнению с Airflow:
- Преимущества:
  - интеграция с Kubernetes, хорошая поддержка контейнеризации
  - масштабируется и управляется в Kubernetes
  - Идеален для ML workflow, где есть этапы моделирования, обучения и развертывания

- Недостатки:
  - Более сложен в установке и настройке
  - Менее универсален для классических ETL задач
  - Меньше сообщество и экосистема, специализируется на ML

## 5. Недостатки, ограничения, риски
- если технология новая необходимо ее осваивать
- усложнение архитектуры 
- дополнительные ресурсы на поддержку(люди, железо)
- необходимо тщательно подходить к расчету потребления ресурсов: памяти и cpu

